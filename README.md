# VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices
[[Project Page]](https://ipcv.github.io/VocaLiST/) [[Arxiv]](https://arxiv.org/abs/2204.02090) [[Weights]](https://drive.google.com/drive/folders/1-g4qHUNNcCZpmSqEflKMxPMvwnn9e88N?usp=sharing)

Official repository for the paper VocaLiST: An Audio-Visual Synchronisation Model for Lips and Voices. 

The paper has been accepted to Interspeech 2022.

### Acknowlegements

Some of the code is borrowed or adapted from Co-Separation. 
The code for the lip analysis network is adapted from 
Lipreading using Temporal Convolutional Networks.

### Licence
This project makes use of source code of other existing works. 
Only the original source code from this repository is provided under CC-BY-NC license, 
while the parts of the repository based on the code reused/adapted from elsewhere 
are available under their respective license terms. 
The code for evaluation setup is adapted from the source code of the paper 
''Out of time: automated lip sync in the wild'' which is made available under https://github.com/joonson/syncnet_python/blob/master/LICENSE.md.

The code for the transformer encoder and its internal components are made available under https://github.com/yaohungt/Multimodal-Transformer/blob/master/LICENSE.

The code related to reading the experiment configuration (hparams) is adapted from the code of https://github.com/Rudrabha/Wav2Lip/
which is under non-commercial license terms. 
